{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43e8786b-4ae1-4606-88be-30067ade1e75",
   "metadata": {},
   "source": [
    "# Multiple-Style Blending Style Transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2027fd3-7cc2-4c7d-bac1-535ee3a95a20",
   "metadata": {},
   "source": [
    "## LIBRARY IMPORTS AND SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "974276cd-ee6c-41e4-ba3a-53c84e4d99be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Style Transfer with Multiple Style Blending\n",
    "# This code implements an advanced neural style transfer system that can blend two style images\n",
    "# with a content image using deep learning techniques and VGG19 neural network\n",
    "\n",
    "# Import libraries\n",
    "# Deep learning and numerical operations\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageEnhance\n",
    "import time\n",
    "\n",
    "# IPython widgets for interactive UI\n",
    "from ipywidgets import (VBox, HBox, FloatSlider, IntSlider, FileUpload, Button, Output, Label, \n",
    "                        Layout, Image as ipyImage, Dropdown, Checkbox)\n",
    "from IPython.display import clear_output, display\n",
    "import io\n",
    "import math\n",
    "\n",
    "# Configure TensorFlow Hub to use compressed model format for faster loading\n",
    "os.environ['TFHUB_MODEL_LOAD_FORMAT'] = 'COMPRESSED'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eeb08a0-b082-4741-ae52-e977726d7b5c",
   "metadata": {},
   "source": [
    "## USER INTERFACE WIDGET CREATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d54167c2-9f36-4928-81d7-22723a4c05e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File upload widgets for content and style images\n",
    "content_upload = FileUpload(description=\"Upload Content\", accept='image/*')\n",
    "style_upload1 = FileUpload(description=\"Upload Style 1\", accept='image/*')\n",
    "style_upload2 = FileUpload(description=\"Upload Style 2\", accept='image/*')\n",
    "\n",
    "# Output widgets to display image previews\n",
    "content_preview = Output()\n",
    "style_preview1 = Output()\n",
    "style_preview2 = Output()\n",
    "\n",
    "# Style weight sliders\n",
    "# Create sliders for adjust style weight\n",
    "# The weights are constrained to sum to 1.0 for proper blending\n",
    "style_slider1 = FloatSlider(\n",
    "    min=0, max=1, value=0.5, step=0.01,\n",
    "    description='Style 1 Weight',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout={'width': '400px'}\n",
    ")\n",
    "\n",
    "style_slider2 = FloatSlider(\n",
    "    min=0, max=1, value=0.5, step=0.01,\n",
    "    description='Style 2 Weight',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout={'width': '400px'}\n",
    ")\n",
    "\n",
    "# Let the sliders must sum to 1.0\n",
    "def update_slider1(change):\n",
    "    style_slider2.value = 1.0 - change['new']\n",
    "\n",
    "def update_slider2(change):\n",
    "    style_slider1.value = 1.0 - change['new']\n",
    "\n",
    "# Attach observers to sliders for automatic synchronization\n",
    "style_slider1.observe(update_slider1, names='value')\n",
    "style_slider2.observe(update_slider2, names='value')\n",
    "\n",
    "# Loss weight sliders\n",
    "# Content weight controls how much the output resembles the original content structure\n",
    "content_weight_slider = FloatSlider(\n",
    "    min=0, max=1, value=1.0, step=0.01,\n",
    "    description='Content Weight (1e4)', # Actual weight is value * 1e4\n",
    "    style={'description_width': 'initial'},\n",
    "    layout={'width': '400px'}\n",
    ")\n",
    "\n",
    "# Style weight controls how much the output adopts the artistic style\n",
    "style_weight_slider = FloatSlider(\n",
    "    min=0, max=1, value=1.0, step=0.01,\n",
    "    description='Style Weight (1e-2)', # Actual weight is value * 1e-2\n",
    "    style={'description_width': 'initial'},\n",
    "    layout={'width': '400px'}\n",
    ")\n",
    "\n",
    "# Total variation weight helps reduce noise and create smoother results\n",
    "tv_weight_slider = FloatSlider(\n",
    "    min=0, max=1, value=1.0, step=0.01,\n",
    "    description='Total Variation (30)',  # Actual weight is value * 30\n",
    "    style={'description_width': 'initial'},\n",
    "    layout={'width': '400px'}\n",
    ")\n",
    "\n",
    "# Training parameter sliders\n",
    "# Epoch\n",
    "epoch_slider = IntSlider(\n",
    "    min=1, max=50, value=10,\n",
    "    description='Epochs',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout={'width': '400px'}\n",
    ")\n",
    "\n",
    "# Step per epoch\n",
    "steps_slider = IntSlider(\n",
    "    min=10, max=200, value=50,\n",
    "    description='Steps/Epoch',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout={'width': '400px'}\n",
    ")\n",
    "\n",
    "# Color preservation slider\n",
    "# Helps maintain the original content's color scheme\n",
    "color_preserve_slider = FloatSlider(\n",
    "    min=0, max=1, value=0.0, step=0.01,\n",
    "    description='Color Preserve',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout={'width': '400px'}\n",
    ")\n",
    "\n",
    "# Output format selection\n",
    "output_quality_dropdown = Dropdown(\n",
    "    options=['JPEG (Default)', 'PNG (Lossless)'],\n",
    "    value='JPEG (Default)',\n",
    "    description='Format:'\n",
    ")\n",
    "\n",
    "# Style morphing creates a gradual transition between styles over epochs\n",
    "style_morphing_toggle = Checkbox(\n",
    "    description='Enable Style Morphing (Style 1 = 0.0 -> 1.0, Style 2 = 1.0 -> 0.0)',\n",
    "    value=False,\n",
    "    layout={'width': '500px'}\n",
    ")\n",
    "\n",
    "# Image enhancement sliders for post-processing\n",
    "brightness_slider = FloatSlider(\n",
    "    min=0.5, max=1.5, value=1.0, step=0.01,\n",
    "    description='Brightness',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout={'width': '400px'}\n",
    ")\n",
    "\n",
    "contrast_slider = FloatSlider(\n",
    "    min=0.5, max=1.5, value=1.0, step=0.01,\n",
    "    description='Contrast',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout={'width': '400px'}\n",
    ")\n",
    "\n",
    "saturation_slider = FloatSlider(\n",
    "    min=0.5, max=1.5, value=1.0, step=0.01,\n",
    "    description='Saturation',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout={'width': '400px'}\n",
    ")\n",
    "\n",
    "# Style scale affects the resolution at which style features are extracted\n",
    "style_scale_slider = FloatSlider(\n",
    "    min=0.1, max=2.0, value=1.0, step=0.1,\n",
    "    description='Style Scale',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout={'width': '400px'}\n",
    ")\n",
    "\n",
    "# Output resolution slider\n",
    "resolution_slider = IntSlider(\n",
    "    min=256, max=2048, step=256, value=512,\n",
    "    description='Output Resolution',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout={'width': '400px'}\n",
    ")\n",
    "\n",
    "# Control buttons\n",
    "train_btn = Button(description=\"Start Training\", button_style='success')\n",
    "reset_btn = Button(description=\"Reset All\", button_style='danger') \n",
    "\n",
    "# Output areas for displaying results and progress\n",
    "output_area = Output() # Training progress and messages\n",
    "result_area = Output() # Final results display\n",
    "\n",
    "# Real-time preview widget shows intermediate results during training\n",
    "live_preview = ipyImage(\n",
    "    layout=Layout(width='400px', height='auto')\n",
    ")\n",
    "\n",
    "# Create section headers\n",
    "upload_header = Label(value=\"üìÅ Image Upload: \", style={'font_weight': 'bold', 'font_size': '16px'})\n",
    "style_weights_header = Label(value=\"‚öñÔ∏è Style Weights Adjustment: \", style={'font_weight': 'bold', 'font_size': '16px'})\n",
    "training_header = Label(value=\"‚öôÔ∏è Training Parameters: \", style={'font_weight': 'bold', 'font_size': '16px'})\n",
    "advanced_header = Label(value=\"üéöÔ∏è Advanced Loss Weights:\", style={'font_weight': 'bold', 'font_size': '16px'})\n",
    "color_adjustment_header = Label(value=\"üé® Color Adjustment:\", style={'font_weight': 'bold', 'font_size': '16px'})\n",
    "output_setting_header = Label(value=\"üñ•Ô∏è Output Setting:\", style={'font_weight': 'bold', 'font_size': '16px'})\n",
    "results_header = Label(value=\"üñºÔ∏è Results: \", style={'font_weight': 'bold', 'font_size': '16px'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83b542f-70a1-4d71-a62a-7f4f95b56b4f",
   "metadata": {},
   "source": [
    "## USER INTERFACE LAYOUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a304f67a-de44-45f4-878c-2321c95d1c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UI Layout\n",
    "ui = VBox([\n",
    "    # Image upload section\n",
    "    VBox([\n",
    "        upload_header,\n",
    "        HBox([\n",
    "            VBox([Label(\"Content Image:\"), content_upload, content_preview]),\n",
    "            VBox([Label(\"Style Image 1:\"), style_upload1, style_preview1]),\n",
    "            VBox([Label(\"Style Image 2:\"), style_upload2, style_preview2])\n",
    "        ])\n",
    "    ], layout=Layout(margin='0 0 20px 0')),\n",
    "\n",
    "    # Style weight controls section\n",
    "    VBox([\n",
    "        style_weights_header,\n",
    "        HBox([style_slider1, style_slider2], layout={'width': '100%'}),\n",
    "        HBox([style_morphing_toggle])\n",
    "    ], layout=Layout(margin='0 0 20px 0')),\n",
    "\n",
    "    # Training parameters section\n",
    "    VBox([\n",
    "        training_header,\n",
    "        HBox([epoch_slider, steps_slider], layout={'width': '100%'})\n",
    "    ], layout=Layout(margin='0 0 20px 0')),\n",
    "\n",
    "    # Advanced loss weight controls section\n",
    "    VBox([\n",
    "        advanced_header,\n",
    "        HBox([content_weight_slider, style_weight_slider]),\n",
    "        HBox([tv_weight_slider, color_preserve_slider]),\n",
    "    ], layout=Layout(margin='0 0 20px 0')),\n",
    "\n",
    "    # Color and style adjustment section\n",
    "    VBox([\n",
    "        color_adjustment_header,\n",
    "        HBox([brightness_slider, contrast_slider]),\n",
    "        HBox([saturation_slider, style_scale_slider]),\n",
    "    ], layout=Layout(margin='0 0 20px 0')),\n",
    "\n",
    "    # Output settings section\n",
    "    VBox([\n",
    "        output_setting_header,\n",
    "        HBox([resolution_slider, output_quality_dropdown])\n",
    "    ], layout=Layout(margin='0 0 20px 0')),\n",
    "\n",
    "    # Control buttons\n",
    "    HBox([train_btn, reset_btn]), \n",
    "\n",
    "    # Results display section\n",
    "    VBox([\n",
    "        results_header,\n",
    "        output_area,\n",
    "        live_preview,\n",
    "        result_area\n",
    "    ])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb27255-9236-41d5-97ab-c296d827f6b6",
   "metadata": {},
   "source": [
    "## IMAGE PREVIEW FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84d93e13-5163-4e65-8afa-40bf49273105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a callback function to update the image preview when the file is uploaded\n",
    "def update_preview(upload_widget, preview_area, max_size=(200, 200)):\n",
    "    def callback(change):\n",
    "        with preview_area:\n",
    "            clear_output() # Clear previous preview\n",
    "            if upload_widget.value:\n",
    "                try:\n",
    "                    # Extract the uploaded file data\n",
    "                    upload_data = upload_widget.value[0]\n",
    "                    content = upload_data['content']\n",
    "\n",
    "                    # Open and resize the image for preview\n",
    "                    img = Image.open(io.BytesIO(content))\n",
    "                    img.thumbnail(max_size) # Resize while maintaining aspect ratio\n",
    "                    display(img)\n",
    "                except Exception as e:\n",
    "                    print(f\"Preview error: {str(e)}\")\n",
    "                    \n",
    "    # Attach the callback to the upload widget\n",
    "    upload_widget.observe(callback, names='value')\n",
    "\n",
    "# Set up preview callbacks for all upload widgets\n",
    "update_preview(content_upload, content_preview)\n",
    "update_preview(style_upload1, style_preview1)\n",
    "update_preview(style_upload2, style_preview2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568f5f52-1453-4e31-b04d-0cadd9c66eaa",
   "metadata": {},
   "source": [
    "## IMAGE PROCESSING FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fca591e1-0d1e-423c-9a46-3812ee7a3fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and process an uploaded image file\n",
    "def process_upload(upload_widget):\n",
    "    if upload_widget.value:\n",
    "        upload_data = upload_widget.value[0]\n",
    "        content = upload_data['content']\n",
    "        # Convert to RGB to ensure consistent color format\n",
    "        img = Image.open(io.BytesIO(content)).convert(\"RGB\")\n",
    "        return np.array(img)\n",
    "    raise ValueError(\"No file uploaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c20f861f-b918-467b-9cf5-cc146ad6861c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess an image for neural style transfer\n",
    "def load_and_preprocess(img_array, max_dim=512):\n",
    "    # Convert to float32 and normalize to [0,1] range\n",
    "    img = tf.image.convert_image_dtype(img_array, tf.float32)\n",
    "\n",
    "    # Calculate scaling factor to fit within max_dim while preserving aspect ratio\n",
    "    shape = tf.cast(tf.shape(img)[:-1], tf.float32) # Get height and width\n",
    "    scale = max_dim / tf.reduce_max(shape) # Scale factor based on larger dimension\n",
    "    new_shape = tf.cast(shape * scale, tf.int32)\n",
    "\n",
    "    # Resize and add batch dimension\n",
    "    return tf.image.resize(img, new_shape)[tf.newaxis, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6f0b243-1b39-4be3-a773-0b92d9f6f5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the color channels of a stylized image to preserve the original content's colors\n",
    "# Use the LAB color space for more uniform color processing\n",
    "def color_adjust(image_np, reference_np, weight):\n",
    "    # Convert numpy arrays to PIL Images\n",
    "    result_img = Image.fromarray(image_np.astype(np.uint8))\n",
    "    content_img = Image.fromarray(reference_np.astype(np.uint8))\n",
    "\n",
    "    # Ensure both images have the same size\n",
    "    if result_img.size != content_img.size:\n",
    "        content_img = content_img.resize(result_img.size, Image.Resampling.LANCZOS)\n",
    "\n",
    "    # Convert to LAB color space for better color manipulation\n",
    "    # LAB separates luminance (L) from color information (A, B)\n",
    "    result_lab = np.array(result_img.convert('LAB')).astype(np.float32)\n",
    "    content_lab = np.array(content_img.convert('LAB')).astype(np.float32)\n",
    "\n",
    "    # Blend the A and B channels (color) while preserving L channel (luminance)\n",
    "    result_lab[:,:,1] = result_lab[:,:,1] * (1 - weight) + content_lab[:,:,1] * weight # A channel\n",
    "    result_lab[:,:,2] = result_lab[:,:,2] * (1 - weight) + content_lab[:,:,2] * weight # B channel\n",
    "\n",
    "    # Clamp values and convert back to RGB\n",
    "    result_lab = np.clip(result_lab, 0, 255).astype(np.uint8)\n",
    "    return np.array(Image.fromarray(result_lab, 'LAB').convert('RGB'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17ebc16c-dbfa-4b5b-8692-cead89630970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply brightness, contrast, and saturation enhancements to image\n",
    "def apply_enhancements(image_np, brightness, contrast, saturation):\n",
    "    img = Image.fromarray(image_np.astype(np.uint8))\n",
    "\n",
    "    # Apply enhancements sequentially using PIL's ImageEnhance\n",
    "    img = ImageEnhance.Brightness(img).enhance(brightness)\n",
    "    img = ImageEnhance.Contrast(img).enhance(contrast)\n",
    "    img = ImageEnhance.Color(img).enhance(saturation)\n",
    "    \n",
    "    return np.array(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ee3b28-f394-4360-9f93-f873448481b8",
   "metadata": {},
   "source": [
    "## NEURAL STYLE TRANSFER CORE CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e407e8f1-1d82-45ec-a7ca-c82f500670ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Style Transfer, Keras model to extracts both style and content features from VGG19\n",
    "class StyleContentModel(tf.keras.models.Model):\n",
    "    # Initialize the style-content extraction model\n",
    "    def __init__(self, vgg, style_layers, content_layers):\n",
    "        super().__init__()\n",
    "        # Create a model that outputs activations from specified layers\n",
    "        self.vgg = tf.keras.Model([vgg.input],\n",
    "                                 [vgg.get_layer(name).output for name in style_layers + content_layers])\n",
    "\n",
    "        # Store layer information\n",
    "        self.style_layers = style_layers\n",
    "        self.content_layers = content_layers\n",
    "        self.num_style_layers = len(style_layers)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Preprocess inputs for VGG19 (scale to [0,255] and apply VGG preprocessing)\n",
    "        inputs = inputs * 255.0\n",
    "        preprocessed_input = tf.keras.applications.vgg19.preprocess_input(inputs)\n",
    "\n",
    "        # Extract features from all specified layers\n",
    "        outputs = self.vgg(preprocessed_input)\n",
    "\n",
    "        # Split outputs into style and content features\n",
    "        style_outputs, content_outputs = (outputs[:self.num_style_layers], outputs[self.num_style_layers:])\n",
    "        style_outputs = [self.gram_matrix(style_output) for style_output in style_outputs]\n",
    "       \n",
    "        return {\n",
    "            'style': {name: value for name, value in zip(self.style_layers, style_outputs)},\n",
    "            'content': {name: value for name, value in zip(self.content_layers, content_outputs)}\n",
    "        }\n",
    "\n",
    "    # Compute Gram matrix for style representation\n",
    "    def gram_matrix(self, input_tensor):\n",
    "        # Compute correlations between feature maps using Einstein summation\n",
    "        result = tf.linalg.einsum('bijc,bijd->bcd', input_tensor, input_tensor)\n",
    "\n",
    "        # Normalize by the size of the feature maps\n",
    "        input_shape = tf.shape(input_tensor)\n",
    "        return result / tf.cast(input_shape[1]*input_shape[2], tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9cfe0aa-e245-42ab-918b-c147ef5a7358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The main NST system that coordinates the entire process\n",
    "class StyleTransferSystem:\n",
    "    def __init__(self):\n",
    "        # Initialize the style transfer system with predefined layer selections\n",
    "        # Deep layers that capture semantic content\n",
    "        self.content_layers = ['block5_conv2']\n",
    "\n",
    "        # Multiple layers at different scales for rich style representation\n",
    "        self.style_layers = ['block1_conv1', # Low-level features (edges, textures)\n",
    "                             'block2_conv1', # Mid-level features\n",
    "                             'block3_conv1', # Higher-level patterns\n",
    "                             'block4_conv1', # Complex patterns\n",
    "                             'block5_conv1'] # High-level features\n",
    "\n",
    "        # Build the feature extraction model\n",
    "        self.extractor = self.build_extractor()\n",
    "\n",
    "    # Build the feature extraction model using pre-trained VGG19\n",
    "    def build_extractor(self):\n",
    "        # Load pre-trained VGG19 without the classification head\n",
    "        vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n",
    "        vgg.trainable = False\n",
    "        return StyleContentModel(vgg, self.style_layers, self.content_layers)\n",
    "\n",
    "    # Execute the neural style transfer optimization process\n",
    "    def run_transfer(self, content_img, style_imgs, weights_list, epochs, steps, content_weight, style_weight, tv_weight):\n",
    "        # Extract target features from style and content images\n",
    "        style_targets = [self.extractor(style_img)['style'] for style_img in style_imgs]\n",
    "        content_targets = self.extractor(content_img)['content']\n",
    "\n",
    "        # Initialize the generated image as a copy of the content image\n",
    "        image = tf.Variable(content_img)\n",
    "        \n",
    "        # Use Adam optimizer for stable convergence\n",
    "        opt = tf.keras.optimizers.Adam(0.02)\n",
    "    \n",
    "        # Clear the live preview and result area before starting\n",
    "        live_preview.value = b''\n",
    "        with result_area:\n",
    "            clear_output()\n",
    "            \n",
    "        with output_area:\n",
    "            print(\"Live Preview:\")\n",
    "\n",
    "        # Optimization loop\n",
    "        for epoch in range(epochs):\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Get style weights for current epoch (enables style morphing)\n",
    "            current_weights = weights_list[epoch]\n",
    "\n",
    "            # Blend multiple style targets according to current weights\n",
    "            blended_style = {\n",
    "                layer: sum(current_weights[i] * style[layer] for i, style in enumerate(style_targets))\n",
    "                for layer in self.style_layers\n",
    "            }\n",
    "\n",
    "            # Optimization steps within each epoch\n",
    "            for step in range(steps):\n",
    "                # Compute gradients using automatic differentiation\n",
    "                with tf.GradientTape() as tape:\n",
    "                    # Extract features from current generated image\n",
    "                    outputs = self.extractor(image)\n",
    "\n",
    "                    # Minimize difference between generated and target style features\n",
    "                    style_loss = tf.add_n([\n",
    "                        tf.reduce_mean((outputs['style'][layer] - blended_style[layer])**2)\n",
    "                        for layer in self.style_layers]) * style_weight * 1e-2\n",
    "\n",
    "                    # Preserve semantic content from original image\n",
    "                    content_loss = tf.add_n([\n",
    "                        tf.reduce_mean((outputs['content'][layer] - content_targets[layer])**2)\n",
    "                        for layer in self.content_layers]) * content_weight * 1e4\n",
    "\n",
    "                    # Encourage spatial smoothness (reduce noise)\n",
    "                    tv_loss = tf.image.total_variation(image) * tv_weight * 30\n",
    "\n",
    "                    # Combined loss function\n",
    "                    total_loss = style_loss + content_loss + tv_loss\n",
    "\n",
    "                # Apply gradients to update the generated image\n",
    "                grad = tape.gradient(total_loss, image)\n",
    "                opt.apply_gradients([(grad, image)])\n",
    "\n",
    "                # Clamp pixel values to valid range [0,1]\n",
    "                image.assign(tf.clip_by_value(image, 0.0, 1.0))\n",
    "\n",
    "                # Display progress information\n",
    "                with output_area:\n",
    "                    clear_output(wait=True)\n",
    "                    print(f\"Epoch {epoch+1}/{epochs} - Step {step+1}/{steps}\")\n",
    "                    print(f\"Style Loss: {style_loss.numpy():.2f} | Content Loss: {content_loss.numpy():.2f}\")\n",
    "                    # print(f\"Epoch {epoch+1} completed in {time.time() - start_time:.1f} seconds\") # Testing Purpose\n",
    "\n",
    "            # Update live preview at the end of each epoch\n",
    "            current_result_pil = Image.fromarray(np.clip(image.numpy()[0] * 255, 0, 255).astype(np.uint8))\n",
    "\n",
    "            # Convert to JPEG for display\n",
    "            buf = io.BytesIO()\n",
    "            current_result_pil.save(buf, format='JPEG')\n",
    "            live_preview.value = buf.getvalue()\n",
    "            \n",
    "        return image.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03f898d-c910-4482-b4ee-a5c43cda62df",
   "metadata": {},
   "source": [
    "## UI EVENT HANDLERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36b46232-43be-4a73-be92-66539149d2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_reset_click(btn):\n",
    "    # Reset all sliders to their initial values\n",
    "    style_slider1.value = 0.5\n",
    "    style_slider2.value = 0.5\n",
    "    content_weight_slider.value = 1.0\n",
    "    style_weight_slider.value = 1.0\n",
    "    tv_weight_slider.value = 1.0\n",
    "    color_preserve_slider.value = 0.0\n",
    "    epoch_slider.value = 10\n",
    "    steps_slider.value = 50\n",
    "    style_morphing_toggle.value = False\n",
    "    brightness_slider.value = 1.0\n",
    "    contrast_slider.value = 1.0\n",
    "    saturation_slider.value = 1.0\n",
    "    style_scale_slider.value = 1.0\n",
    "    resolution_slider.value = 512\n",
    "    output_quality_dropdown.value = 'JPEG (Default)'\n",
    "\n",
    "    print(\"Settings and outputs have been reset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57115598-825d-4f54-a1c3-550b137eef4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UI Event Handlers\n",
    "def on_train_click(btn):\n",
    "    with output_area:\n",
    "        clear_output()\n",
    "        print(\"Starting style transfer...\")\n",
    "\n",
    "    try:\n",
    "        # Validate that all required images are uploaded\n",
    "        if not (content_upload.value and style_upload1.value and style_upload2.value):\n",
    "            raise ValueError(\"Please upload all required images\")\n",
    "\n",
    "        # Process uploaded images\n",
    "        content_img = process_upload(content_upload)\n",
    "        style_img1 = process_upload(style_upload1)\n",
    "        style_img2 = process_upload(style_upload2)\n",
    "\n",
    "        # Extract parameter values from UI controls\n",
    "        content_w = content_weight_slider.value\n",
    "        style_w = style_weight_slider.value\n",
    "        tv_w = tv_weight_slider.value\n",
    "        color_w = color_preserve_slider.value\n",
    "        brightness = brightness_slider.value\n",
    "        contrast = contrast_slider.value\n",
    "        saturation = saturation_slider.value\n",
    "        style_scale = style_scale_slider.value\n",
    "        resolution = resolution_slider.value\n",
    "\n",
    "        # Determine style blending weights for each epoch\n",
    "        if style_morphing_toggle.value:\n",
    "            # Gradually transition from style 1 to style 2\n",
    "            total_epochs = epoch_slider.value\n",
    "            weights = []\n",
    "            for i in range(total_epochs):\n",
    "                morph_factor = i / (total_epochs - 1) if total_epochs > 1 else 0.0\n",
    "                w1 = 1.0 - morph_factor # Decreases from 0.0 to 1.0\n",
    "                w2 = morph_factor # Increases from 1.0 to 0.0\n",
    "                weights.append([w1, w2])\n",
    "        else:\n",
    "            # Training based on weight sliders\n",
    "            w1 = style_slider1.value\n",
    "            w2 = style_slider2.value\n",
    "            weights = [[w1, w2]] * epoch_slider.value\n",
    "\n",
    "        # Preprocess images for neural network input\n",
    "        content_tensor = load_and_preprocess(content_img, max_dim=resolution)\n",
    "        style_tensors = [\n",
    "            load_and_preprocess(style_img1, max_dim=int(resolution * style_scale)),\n",
    "            load_and_preprocess(style_img2, max_dim=int(resolution * style_scale))\n",
    "        ]\n",
    "\n",
    "        # Display input images for reference\n",
    "        with output_area:\n",
    "            clear_output()\n",
    "            fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "            axes[0].imshow(content_img)\n",
    "            axes[0].set_title('Content Image')\n",
    "            axes[1].imshow(style_img1)\n",
    "            axes[1].set_title(f'Style 1 (Weight: {w1:.0%})')\n",
    "            axes[2].imshow(style_img2)\n",
    "            axes[2].set_title(f'Style 2 (Weight: {w2:.0%})')\n",
    "            plt.show()\n",
    "            print(\"Starting style transfer...\")\n",
    "\n",
    "        # Execute the neural style transfer\n",
    "        sts = StyleTransferSystem()\n",
    "        result = sts.run_transfer(content_tensor, style_tensors, weights,\n",
    "                                 epoch_slider.value, steps_slider.value,\n",
    "                                 content_w, style_w, tv_w)\n",
    "\n",
    "        # Clear the live preview after training is finished\n",
    "        live_preview.value = b''\n",
    "\n",
    "        # Convert result to displayable format\n",
    "        result_img = np.clip(result[0] * 255, 0, 255).astype(np.uint8)\n",
    "\n",
    "        # Apply color preservation if requested\n",
    "        if color_w > 0:\n",
    "            # Resize content image to match result dimensions\n",
    "            content_resized = tf.image.resize(content_img[tf.newaxis, :],\n",
    "                                             [result_img.shape[0], result_img.shape[1]])\n",
    "            content_resized = tf.clip_by_value(content_resized[0], 0, 255).numpy().astype(np.uint8)\n",
    "            stylized_img = color_adjust(result_img, content_resized, color_w)\n",
    "        else:\n",
    "            stylized_img = result_img\n",
    "\n",
    "        # Apply final enhancements (brightness, contrast, saturation)\n",
    "        final_img = apply_enhancements(stylized_img, brightness, contrast, saturation)\n",
    "\n",
    "        # Display final results\n",
    "        with result_area:\n",
    "            clear_output()\n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "            ax1.imshow(result_img)\n",
    "            ax1.set_title('Stylized Result (Raw)')\n",
    "            ax2.imshow(final_img)\n",
    "            ax2.set_title('Color-Preserved & Enhanced Result')\n",
    "            plt.show()\n",
    "\n",
    "            # Save results to files\n",
    "            file_format = \"png\" if output_quality_dropdown.value == 'PNG (Lossless)' else \"jpg\"\n",
    "            Image.fromarray(result_img).save(f\"result_raw.{file_format}\")\n",
    "            Image.fromarray(final_img).save(f\"result_final.{file_format}\")\n",
    "            print(f\"Results saved as result_raw.{file_format} and result_final.{file_format}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle and display any errors that occur during processing\n",
    "        with output_area:\n",
    "            clear_output()\n",
    "            print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a92e1e3-92d2-4178-a18d-9a5bc9e9d70e",
   "metadata": {},
   "source": [
    "## UI INITIALIZATION AND STARTUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db3c8c87-fa86-4388-855a-117e9115f39d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36b3fea16b4d407da31c008eb59162b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(VBox(children=(Label(value='üìÅ Image Upload: ', style=LabelStyle(font_size='16px', font_weight='‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Start UI\n",
    "train_btn.on_click(on_train_click) # Start training when the train button is clicked\n",
    "reset_btn.on_click(on_reset_click) # Reset all settings when the reset button is clicked\n",
    "\n",
    "# Display the complete user interface\n",
    "display(ui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e40980-a401-47fc-82b2-ec257b8efb4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
